{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 18:06:08 WARN Utils: Your hostname, Zhijie resolves to a loopback address: 127.0.1.1; using 10.27.109.93 instead (on interface wlo1)\n",
      "24/12/13 18:06:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/13 18:06:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def load_and_preprocess(file_path, schema):\n",
    "    return spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .option(\"inferSchema\", \"false\") \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(file_path) \\\n",
    "        .drop(\"_index\", \"_arrive\", \"_creation\", \"_device\")\n",
    "\n",
    "def compute_group_statistics(data):\n",
    "    return data.groupBy(data['user'], data['model'], data['gt']).agg(\n",
    "        F.mean(data['x']).alias('mean_x'),\n",
    "        F.mean(data['y']).alias('mean_y'),\n",
    "        F.mean(data['z']).alias('mean_z'),\n",
    "        F.stddev(data['x']).alias('std_x'),\n",
    "        F.stddev(data['y']).alias('std_y'),\n",
    "        F.stddev(data['z']).alias('std_z'),\n",
    "        F.max(data['x']).alias('max_x'),\n",
    "        F.max(data['y']).alias('max_y'),\n",
    "        F.max(data['z']).alias('max_z'),\n",
    "        F.min(data['x']).alias('min_x'),\n",
    "        F.min(data['y']).alias('min_y'),\n",
    "        F.min(data['z']).alias('min_z')\n",
    "    )\n",
    "\n",
    "fields = [\n",
    "    StructField(\"_index\", BooleanType(), False), StructField(\"_arrive\", BooleanType(), False), StructField(\"_creation\", BooleanType(), False),\n",
    "    StructField(\"x\", DoubleType(), False), StructField(\"y\", DoubleType(), False), StructField(\"z\", DoubleType(), False),\n",
    "    StructField(\"user\", StringType(), False), StructField(\"model\", StringType(), False), StructField(\"_device\", StringType(), False),\n",
    "    StructField(\"gt\", StringType(), False)\n",
    "]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = spark.read.format(\"csv\").option(header=False, inferScheme=False, delimiter=\",\").schema(schema).load(\"Phones_accelerometer.csv\")\n",
    "# data1 = spark.read.csv(\"Phones_accelerometer.csv\", schema=schema, nullValue=\"null\")\n",
    "\n",
    "data1 = load_and_preprocess(\"Phones_accelerometer.csv\", schema)\n",
    "data2 = load_and_preprocess(\"Phones_gyroscope.csv\", schema)\n",
    "data3 = load_and_preprocess(\"Watch_accelerometer.csv\", schema)\n",
    "data4 = load_and_preprocess(\"Watch_gyroscope.csv\", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = compute_group_statistics(data1)\n",
    "group2 = compute_group_statistics(data2)\n",
    "group3 = compute_group_statistics(data3)\n",
    "group4 = compute_group_statistics(data4)\n",
    "\n",
    "join1 = group1.join(group2, ['user', 'model', 'gt'], 'inner')\n",
    "join2 = group3.join(group4, on=['user', 'model', 'gt'], how='inner')\n",
    "\n",
    "results = join1.union(join2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 18:06:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+------------------+-------------------+-------------------+--------------------+--------------------+-------------------+----------+------------------+----------+-----------+-----------+----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+------------+-----------+-----------+-------------------+\n",
      "|user| model|   gt|            mean_x|             mean_y|             mean_z|               std_x|               std_y|              std_z|     max_x|             max_y|     max_z|      min_x|      min_y|     min_z|              mean_x|              mean_y|              mean_z|               std_x|              std_y|               std_z|             max_x|              max_y|       max_z|      min_x|      min_y|              min_z|\n",
      "+----+------+-----+------------------+-------------------+-------------------+--------------------+--------------------+-------------------+----------+------------------+----------+-----------+-----------+----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+------------+-----------+-----------+-------------------+\n",
      "|   a|nexus4|stand| -6.02649995057012| 0.9334959509016011|  8.013646013119995|  0.1845609750147674| 0.24044618153707975|0.17600865886717884|-5.5202026|1.9472808999999998|  8.638794| -7.0448303|-0.84251404|  7.149872|0.001588851949095...|0.001009460465647035|4.421844293490051...|0.042777065961724134|0.02861444674545153|0.045943341281070235|0.6321869000000001|0.34971620000000003|  0.44873047|-0.16569519|-0.15550232|-0.6001586999999999|\n",
      "|   a|  gear| null|-9.260130103197667|-3.4726260017441857|-1.0407208369767433|0.031136843450928816|0.051533902451264935|0.03147594360906463| -9.173983|        -3.3207579|-0.9307459| -9.3583355| -3.5942953|-1.1420342|0.022570096625655994|-0.03660876029446...|-0.07169493794752187|0.017825868007484237| 0.0115782312798891|0.008051421311796689|         0.0561927|       -0.018109497|-0.055127434| -0.0314253|-0.08042747|        -0.09347696|\n",
      "|   a|  gear|stand| -9.28975320121918|-3.1371565242007433|-1.0662863009969377|  0.4137450252528032|  1.0629582543131455|  0.617385096584336|-0.5650316|        -0.5781997|   1.01574| -12.600683|  -11.08276|-2.2625206|0.022146662344973197|-0.03286732380555572|-0.07143672518383175| 0.07315669446268228|0.05570795773485951| 0.05018246766174032|        0.81039995|         0.35446674|   1.1475562| -2.0383835|-0.55287224|         -1.2319783|\n",
      "|   a|  gear|  sit| -7.60451970918391| -5.528341078676799| 2.6383265126877076| 0.18557251182385523| 0.27430079447562067| 0.3857223300994913|-6.6588736|       -0.83258367|  3.555988|-10.8229885| -6.9168487|-3.6709096| 0.02270561507082863|-0.04142249651518432|-0.06997320145904558|0.057039875079776335|0.05627534029912766| 0.04524252844326984|        0.39867523|         0.39920786|  0.77604514|  -0.700944|-0.79042625|        -0.22849922|\n",
      "+----+------+-----+------------------+-------------------+-------------------+--------------------+--------------------+-------------------+----------+------------------+----------+-----------+-----------+----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+------------+-----------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 18:06:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
